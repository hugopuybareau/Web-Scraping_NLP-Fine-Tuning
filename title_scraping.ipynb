{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "\n",
    "import requests # HTTP requests \n",
    "from bs4 import BeautifulSoup # Extract HTML content\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: FBI probes whether Silicon Valley venture firm passed secrets to China\n",
      "Link: https://www.ft.com/content/d94a5467-ebf9-4992-af13-3e71061707a4\n",
      "Category: Industrial espionage\n",
      "----------------------------------------\n",
      "Title: Google files Brussels complaint against Microsoft cloud business\n",
      "Link: https://www.ft.com/content/65567a16-434c-4865-9098-2cc8a0c76f68\n",
      "Category: EU tech regulation\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple scraper\n",
    "\n",
    "def scrape_tech_news():\n",
    "\n",
    "    url = 'https://www.ft.com/technology'\n",
    "\n",
    "    # HTTP request to scrap page information\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check response status\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Ã‰chec du scraping : {response.status_code}\")\n",
    "        return\n",
    "    \n",
    "    # HTML content analysis\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the article listed on the page\n",
    "    articles = soup.find_all('div', class_='o-teaser__content')\n",
    "    \n",
    "    news = []\n",
    "    \n",
    "    for article in articles:\n",
    "        # Title and link extraction\n",
    "        article_heading = article.find('a', class_='js-teaser-heading-link')\n",
    "        title = article_heading.get_text(strip=True) if article_heading else \"No title\"\n",
    "        link = \"https://www.ft.com\" + article_heading['href'] if article_heading else \"No link\"\n",
    "        \n",
    "        # Extract article tag to define categories\n",
    "        article_tag = article.find('a', class_='o-teaser__tag')\n",
    "        tag = article_tag['aria-label'] if article_tag else 'No cat'\n",
    "        tag = tag.replace('Category: ', '')\n",
    "        \n",
    "        news.append({\n",
    "            'title': title,\n",
    "            'link': link,\n",
    "            'tag': tag,\n",
    "        })\n",
    "    \n",
    "    return news\n",
    "\n",
    "ft_news_scraped = scrape_tech_news()\n",
    "\n",
    "# Print\n",
    "for article in ft_news_scraped[0:2]:\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Link: {article['link']}\")\n",
    "    print(f\"Category: {article['tag']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model to generate text \n",
    "\n",
    "from openai import OpenAI # I won't use openai cause there is a limit for the requests. \n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer # Prompt issue\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key = \"xxx\"\n",
    ")\n",
    "\n",
    "# # T5 model\n",
    "# model_name = 't5-base'\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Bart model\n",
    "model_name = \"facebook/bart-large-cnn\" # already trained\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: FBI probes whether Silicon Valley venture firm passed secrets to China\n",
      "Link: https://www.ft.com/content/d94a5467-ebf9-4992-af13-3e71061707a4\n",
      "Category: Industrial espionage\n",
      "Content: FBI probes whether Silicon Valley venture firm passed secrets to China. \n",
      "FBI probing whether venture firm pass secrets to Chinese government. \n",
      "FBI also probing whether firm helped Chinese government spy on U.S. \n",
      "citizens. \n",
      "FBI investigating whether firm assisted Chinese government in spying on American citizens in Silicon Valley.\n",
      "\n",
      " ==================================================================================================== \n",
      "\n",
      "Title: Google files Brussels complaint against Microsoft cloud business\n",
      "Link: https://www.ft.com/content/65567a16-434c-4865-9098-2cc8a0c76f68\n",
      "Category: EU tech regulation\n",
      "Content: Google files Brussels complaint against Microsoft cloud business. \n",
      "Google says Microsoft's cloud business is a threat to its business model. \n",
      "Microsoft says it has no plans to change its cloud business model in the near future. \n",
      "The company says it will continue to provide cloud services to customers in the future.\n",
      "\n",
      " ==================================================================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generation function\n",
    "\n",
    "def generate_content_from_title(title):\n",
    "    prompt = f\"Write a tech article on this subject : {title}.\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2000, truncation=True)\n",
    "\n",
    "    outputs = model.generate(inputs.input_ids, max_length=2000, num_beams=4)\n",
    "\n",
    "    # Decrypt generated text\n",
    "    article = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return article\n",
    "\n",
    "\n",
    "for article in ft_news_scraped[0:2]:\n",
    "    title = article['title']\n",
    "    # tag = article['tag']\n",
    "    article['content'] = generate_content_from_title(title)\n",
    "    article['content'] = article['content'].replace('. ', '. \\n') # Output ergonomy\n",
    "\n",
    "# Print\n",
    "for article in ft_news_scraped[0:2]:\n",
    "    print(f\"Title: {article['title']}\")\n",
    "    print(f\"Link: {article['link']}\")\n",
    "    print(f\"Category: {article['tag']}\")\n",
    "    print(f\"Content: {article['content']}\")\n",
    "    print('\\n', \"=\" * 100, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
